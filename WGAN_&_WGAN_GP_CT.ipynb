{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "DsMbjobXCv6n",
    "outputId": "77cb740e-2c09-43a7-c9a1-98d18a5f555d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch import autograd\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.distributions as td\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "tMjXvv0l20NP",
    "outputId": "2f354bbd-fde3-44e3-9e77-1d2dbc7ce962"
   },
   "outputs": [],
   "source": [
    "noise = torch.rand(28,28).to(device)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "tl = torch.utils.data.DataLoader(train_data, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PX2t6-6BCv7C"
   },
   "outputs": [],
   "source": [
    "#dataiter = iter(train_loader)\n",
    "#images, labels = dataiter.next()\n",
    "#images = images.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sa4vs7QhCv7W"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, stride = 1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, stride = 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride = 1)            \n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, stride = 2)\n",
    "        #self.conv5 = nn.Conv2d(128, 256, 3, stride = 1)  #MNIST SMALL FOR THIS NTWORK\n",
    "        #self.conv6 = nn.Conv2d(256, 256, 3, stride = 2)  #END UP WITH IMAGE SIZE SMALLER THAN THE KERNEL SIZE \n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)          \n",
    "        self.layer1 = torch.nn.Linear(2048, 1024)\n",
    "        self.layer2 = torch.nn.ReLU()\n",
    "        self.layer3 = torch.nn.Linear(1024, 1)\n",
    "        #self.layer4 = torch.nn.Sigmoid() \n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        out = self.relu(self.conv1(input))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.relu(self.conv3(out))\n",
    "        out = self.relu(self.conv4(out))\n",
    "        #out = self.relu(self.conv5(out))\n",
    "        #out = self.relu(self.conv6(out))\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        #out = self.layer4(out)\n",
    "        \n",
    "        return out\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YyxH1jzkCv7e"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3)\n",
    "        #self.conv3 = nn.Conv2d(32, 32, 3)\n",
    "        #self.conv4 = nn.Conv2d(32, 32, 3)\n",
    "        #self.deconv1 = nn.ConvTranspose2d(32,32,3)\n",
    "        #self.deconv2 = nn.ConvTranspose2d(64,32,3)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64,32,3)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64,1,3)\n",
    "            \n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "      x  = F.relu(self.conv1(x))\n",
    "      x1 = x\n",
    "      x  = F.relu(self.conv2(x))\n",
    "      x2 = x\n",
    "      #x  = F.relu(self.conv3(x))\n",
    "      #x3 = x\n",
    "      #x  = F.relu(self.conv4(x))\n",
    "      #x  = F.relu(self.deconv1(x))\n",
    "      #x  = F.relu(self.deconv2(torch.cat([x3, x], dim=1)))\n",
    "      x  = F.relu(self.deconv3(torch.cat([x2, x], dim=1)))\n",
    "      x  = F.relu(self.deconv4(torch.cat([x1, x], dim=1)))\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HpGxJWTACv7l"
   },
   "outputs": [],
   "source": [
    "lambda_pen=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "EJRjmcosCv7s",
    "outputId": "5082f59b-7089-44ae-b48e-22539966365f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (deconv3): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (deconv4): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate discriminator and generator\n",
    "D = Discriminator()\n",
    "G = Generator()\n",
    "D.to(device)\n",
    "G.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_bUolZUCv75"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Optimizers\n",
    "lr = 0.002\n",
    "\n",
    "# wgan optimizers Create optimizers for the discriminator and generator\n",
    "#D_optimizer = optim.RMSprop(D.parameters(), lr)\n",
    "#G_optimizer = optim.RMSprop(G.parameters(), lr)\n",
    "\n",
    "\n",
    "###### wgan-gp optimizer ########\n",
    "D_optimizer = optim.Adam(D.parameters(), lr)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bQYuVeviCXg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def gauss(args) :\n",
    "  return args + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "AJa1ap8kCv8D",
    "outputId": "d363c45d-c406-4371-f56d-a4e125b3f15f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# training hyperparams\n",
    "num_epochs = 4000\n",
    "num_critic = 10\n",
    "\n",
    "# keep track of loss and generated, \"fake\" samples\n",
    "samples = []\n",
    "losses = []\n",
    "\n",
    "print_every = 400\n",
    "\n",
    "# Get some fixed data for sampling. These are images that are held\n",
    "# constant throughout training, and allow us to inspect the model's performance : LDCT_test\n",
    "\n",
    "x, _ = next(iter(tl))\n",
    "LDCT_test = x.to(device) + noise\n",
    "\n",
    "# uniform = torch.distributions.uniform.Uniform(-1, 1, validate_args=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train the network\n",
    "D.train()\n",
    "G.train()\n",
    "for epoch in range(num_epochs):\n",
    "  \n",
    "    ii = 1\n",
    "    \n",
    "    for batch_i, (ND_CT, _) in enumerate(train_loader):\n",
    "   \n",
    "        # train D\n",
    "        \n",
    "        \n",
    "        NDCT = ND_CT.to(device)\n",
    "        D_optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Train with NDCT\n",
    "\n",
    "        # Compute the discriminator losses on NDCT images \n",
    "        \n",
    "        D_NDCT = D(NDCT)\n",
    "        \n",
    "        \n",
    "        # 2. Train with LDCT images\n",
    "        \n",
    "        # Generate LDCT images\n",
    "        LDCT = gauss(NDCT)\n",
    "        G_LDCT = G(LDCT)\n",
    "        D_LDCT = D(G_LDCT)\n",
    "        \n",
    "        ############# wgan gp ###################\n",
    "        \n",
    "        epsilon = torch.distributions.uniform.Uniform(0, 1, validate_args=None).sample()\n",
    "            \n",
    "        interpolated = epsilon * NDCT + (1 - epsilon) * G_LDCT \n",
    "        \n",
    "        \n",
    "        gradients = autograd.grad(outputs=D(interpolated), inputs=interpolated,\n",
    "                               grad_outputs=torch.ones(D(interpolated).size()).to(device),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "             \n",
    "\n",
    "        grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_pen\n",
    "        \n",
    "        \n",
    "        D_loss = -D_LDCT.mean() + D_NDCT.mean() + grad_penalty\n",
    "            \n",
    "                        \n",
    "        # wgan loss :  D_loss = -(torch.mean(D_real) - torch.mean(D_fake))\n",
    "        D_loss.backward(retain_graph=True)\n",
    "        D_optimizer.step()\n",
    "                                         \n",
    "        # train G\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 1. Train with denoised images and flipped labels\n",
    "        \n",
    "        if ii == num_critic :\n",
    "          \n",
    "          G_optimizer.zero_grad()\n",
    "          \n",
    "          NDCT = ND_CT.to(device)\n",
    "          LDCT = gauss(NDCT)\n",
    "          \n",
    "          # Generate denoised images\n",
    "          G_LDCT = G(LDCT)\n",
    "          D_LDCT = D(G_LDCT)\n",
    "          G_loss = - D_LDCT.mean()\n",
    "        \n",
    "          # perform backprop\n",
    "          G_loss.backward(retain_graph=True)\n",
    "          G_optimizer.step()\n",
    "          ii = 0\n",
    "          \n",
    "        ii = ii + 1  \n",
    "\n",
    "        # Print some loss stats\n",
    "        if batch_i % print_every == 10:\n",
    "            # print discriminator and generator loss\n",
    "            print('Epoch [{:5d}/{:5d}] | D_loss: {:6.4f} | G_loss: {:6.4f}'.format(\n",
    "                    epoch+1, num_epochs, D_loss.item(), G_loss.item()))\n",
    "\n",
    "    \n",
    "    ## after each epoch ##\n",
    "    # append discriminator loss and generator loss\n",
    "    losses.append((D_loss.item(), G_loss.item()))\n",
    "    \n",
    "    # generate and save sample, denoised images\n",
    "    G.eval() # eval mode for generating samples\n",
    "    denoised = G(LDCT_test).cpu()\n",
    "    plt.imshow(denoised.detach()[0].numpy()[0], cmap='gray')\n",
    "    samples.append(denoised)\n",
    "    G.train() # back to train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEtNl15C2ZJL"
   },
   "outputs": [],
   "source": [
    " plt.imshow(LDCT_test[0].cpu().numpy()[0], cmap='gray')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rJXsIYlI2j9N"
   },
   "outputs": [],
   "source": [
    "plt.imshow(x[0].numpy()[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1EUtualp4et2"
   },
   "outputs": [],
   "source": [
    "samples = torch.zeros(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YInVvdwJ4lw7"
   },
   "outputs": [],
   "source": [
    "x, _ = next(iter(tl))\n",
    "LDCT_test = x.to(device) + noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hHOfPscD7hX"
   },
   "outputs": [],
   "source": [
    " plt.imshow(denoised.detach()[0].numpy()[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yxl5QPMMDknf"
   },
   "outputs": [],
   "source": [
    "denoised.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVV2In9LEdET"
   },
   "outputs": [],
   "source": [
    "batch_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPeqUh4bVL5j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WGAN_&_WGAN_GP_CT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
